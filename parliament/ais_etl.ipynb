{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84b0df24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdb_file = r\"D:\\data\\AIS\\global AIS from NOAA\\Zone10_2009_01\\Zone10_2009_01.gdb\"\n",
    "outpath = r\"D:\\data\\AIS\\global AIS from NOAA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91217a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' to make it run - make a req...txt and install later\\nconda install gdal\\npip install h3 polars\\n\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" to make it run - make a req...txt and install later\n",
    "conda install gdal\n",
    "pip install h3 polars\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b521571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _gdal: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clear_output\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mosgeo\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mosgeo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ogr\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\osgeo\\__init__.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m                 fp\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     20\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m _mod\n\u001b[1;32m---> 21\u001b[0m     _gdal \u001b[38;5;241m=\u001b[39m \u001b[43mswig_import_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m swig_import_helper\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\osgeo\\__init__.py:17\u001b[0m, in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m         _mod \u001b[38;5;241m=\u001b[39m \u001b[43mimp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_gdal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpathname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m         fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\imp.py:242\u001b[0m, in \u001b[0;36mload_module\u001b[1;34m(name, file, filename, details)\u001b[0m\n\u001b[0;32m    240\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m load_dynamic(name, filename, opened_file)\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_dynamic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m type_ \u001b[38;5;241m==\u001b[39m PKG_DIRECTORY:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_package(name, filename)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\imp.py:342\u001b[0m, in \u001b[0;36mload_dynamic\u001b[1;34m(name, path, file)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# Issue #24748: Skip the sys.modules check in _load_module_shim;\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m# always load new extension\u001b[39;00m\n\u001b[0;32m    340\u001b[0m spec \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mmachinery\u001b[38;5;241m.\u001b[39mModuleSpec(\n\u001b[0;32m    341\u001b[0m     name\u001b[38;5;241m=\u001b[39mname, loader\u001b[38;5;241m=\u001b[39mloader, origin\u001b[38;5;241m=\u001b[39mpath)\n\u001b[1;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _gdal: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import osgeo\n",
    "from osgeo import ogr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63aa8d",
   "metadata": {},
   "source": [
    "# Read NOAA collected AIS data from url to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65294728",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing ogrext: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfiona\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\fiona\\__init__.py:84\u001b[0m\n\u001b[0;32m     81\u001b[0m     libdir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.libs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPATH\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPATH\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m libdir\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfiona\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BytesCollection, Collection\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfiona\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdrvsupport\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m supported_drivers\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfiona\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ensure_env_with_credentials, Env\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\fiona\\collection.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfiona\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat, vfs\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfiona\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mogrext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Iterator, ItemsIterator, KeysIterator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfiona\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mogrext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Session, WritingSession\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfiona\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mogrext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m buffer_to_virtual_file, remove_virtual_file, GEOMETRY_TYPES\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing ogrext: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import fiona\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f124c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h3\n",
    "import polars as pl\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import urllib.request as urllib2\n",
    "\n",
    "# thanks to https://stackoverflow.com/questions/11023530/python-to-list-http-files-and-directories\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b69dd8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2022/'\n",
    "ext = 'zip'\n",
    "\n",
    "# get the url for each file\n",
    "def listFD(url, ext=''):\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    return [url + '/' + node.get('href') for node in soup.find_all('a') if node.get('href').endswith(ext)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55aee8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add H3 feature and put timestamps in datetime format\n",
    "def pl_h3(pf, lat_col='LAT', lon_col='LON', new_col='H3', resolution=16):\n",
    "    \"\"\"\n",
    "    ### TODO: write a preprocessor function that uses this and re-saves the files\n",
    "    \n",
    "    this converts lat lon to h3 in polars\n",
    "    (function to apply a function to 2 columns in polars....)\n",
    "    \"\"\"\n",
    "    return pf.with_columns(pl.struct([lon_col,lat_col]).apply(lambda x: h3.geo_to_h3(lat=x[lat_col], lng=x[lon_col], resolution=7)).alias(new_col))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c8e421",
   "metadata": {},
   "source": [
    "## Data reading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e356fe91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "# this needs to go inside sav_dat()\n",
    "def read_gdb_from_zip(gdb_file):\n",
    "    \"\"\"\n",
    "    # if its a geodatabase (gdb)\n",
    "    ### Function to read very large gdb files into parquet\n",
    "\n",
    "    \"\"\"\n",
    "    def get_gdb_size(gdb_file):\n",
    "        \"\"\"\n",
    "        credit, modified from: \n",
    "            https://gis.stackexchange.com/questions/205861/get-row-counts-of-all-tables-in-file-geodatabase-ideally-from-metadata\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        #Opens filegdb using ogr driver\n",
    "        ogdb= ogr.Open(gdb_file)                         \n",
    "\n",
    "        #counts no. of feature classes in geodatabase\n",
    "        noOfLyrs = ogdb.GetLayerCount()  \n",
    "\n",
    "        layer = []\n",
    "        rows = []\n",
    "        #loop through feature classes\n",
    "        for fcIdx in range(0, noOfLyrs):              \n",
    "\n",
    "            #gets feature class\n",
    "            fc = ogdb.GetLayer(fcIdx)    \n",
    "\n",
    "            layer.append(fc.GetName())\n",
    "\n",
    "            rows.append(fc.GetFeatureCount())\n",
    "\n",
    "\n",
    "        return list(zip(layer, rows))\n",
    "\n",
    "    # layers = fiona.listlayers(gdb_file)\n",
    "    # print(layers)\n",
    "\n",
    "    import warnings; warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')\n",
    "\n",
    "    step = 100000\n",
    "    lat_col = 'LAT'\n",
    "    lon_col = 'LON'\n",
    "\n",
    "    layers = get_gdb_size(gdb_file)\n",
    "\n",
    "    for layer in layers:\n",
    "\n",
    "        start = 0\n",
    "        stop = step\n",
    "\n",
    "        rnd = 0\n",
    "\n",
    "        while start < layer[-1]:\n",
    "            rnd+=1\n",
    "\n",
    "            # dont read past the end of the file\n",
    "            if stop > layer[-1]:\n",
    "                stop = layer[-1]\n",
    "\n",
    "            print('Processing:', layer[0], '\\n\\t', start, '-', stop)\n",
    "\n",
    "            filename = f\"{egrps['gdb'][0].replace('.gdb', '')}_layer={layer[0]}_range={str(start)+'-'+str(stop)}_h3.parquet\"\n",
    "\n",
    "\n",
    "            print('\\tOutfile:', filename)\n",
    "            if filename in os.listdir(outpath):\n",
    "                print('\\t\\tFile already exists')\n",
    "            else:\n",
    "                try:\n",
    "                    gdf = gpd.read_file(gdb_file, \n",
    "                          rows=slice(start, stop-1), \n",
    "                          engine='fiona',\n",
    "                          layer=layer[0])\n",
    "\n",
    "                    # perform h3 conversion only where there is geometry to use\n",
    "                    if 'geometry' in gdf.columns and not gdf['geometry'].is_null().all():\n",
    "\n",
    "                        # keep only non-null geometries\n",
    "                        gdf = gdf.filter(pl.col('geometry').is_not_null())\n",
    "\n",
    "                        # extract coords from \n",
    "                        gdf[lon_col] = gdf['geometry'].apply(lambda x: x.x)\n",
    "                        gdf[lat_col] = gdf['geometry'].apply(lambda y: y.y)\n",
    "\n",
    "                        # now that we have read it in, ->polars->h3 encode->parquet file\n",
    "                        gdf = pl_h3(gdf, \n",
    "                                    lat_col=lat_col, \n",
    "                                    lon_col=lon_col, \n",
    "                                    new_col='H3', \n",
    "                                    resolution=16)\n",
    "\n",
    "                    else:\n",
    "                        filename = filename.replace('h3', 'NO_COORDS')\n",
    "                        print('\\tNo geometry')\n",
    "                except:\n",
    "                    print('  \\tFailed to process:', layer[0], '\\n\\t', start, stop)\n",
    "\n",
    "                # even if they have no geometry, still save the file\n",
    "                 # this is a crappy way to convert but all i could make work\n",
    "                gdf.to_parquet('file.parquet')\n",
    "                gdf = pl.read_parquet('file.parquet')\n",
    "\n",
    "                gdf.write_parquet(f'{outpath}{os.sep}{filename}')\n",
    "                print('\\tSuccessfully added file')\n",
    "\n",
    "            # set extents to iterate in the file\n",
    "            stop+=step\n",
    "            start+=step\n",
    "\n",
    "            if stop >= layer[-1]:\n",
    "                stop = layer[-1]\n",
    "\n",
    "            clear_output(wait=True)\n",
    "    print('Conversion Complete: ', gdb_file)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadb4bb7",
   "metadata": {},
   "source": [
    "## File reading and parsing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b12dc455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to select process by filetype - to add in above\n",
    "\n",
    "# Get gdb row counts so we can slice\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b88175c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tOpened zip from remote source:\n",
      "egrps {'gdb': ['Zone10_2009_01.gdb']}\n",
      "fgrps []\n",
      "read_to_file() | NAME: ['Zone10_2009_01.gdb']\n",
      "\t\tGDB read\n",
      "gdb Failed\n"
     ]
    }
   ],
   "source": [
    "# https://www.rebasedata.com/python-read-gdb\n",
    "# https://pypi.org/project/poster3/\n",
    "# https://github.com/dmorrison42/python-poster\n",
    "\n",
    "# dont actually need these at the moment but can use to stream it in rather than download the file. maybe\n",
    "from poster3.encode import multipart_encode\n",
    "from poster3.streaminghttp import register_openers\n",
    "\n",
    "def save_dat(url, outpath = r\"D:\\data\\AIS\\global AIS from NOAA\"):\n",
    "\n",
    "    # read a zip file from a url\n",
    "    archive = urllib2.urlopen(url).read()\n",
    "    \n",
    "    # unzip file\n",
    "    archive = zipfile.ZipFile(BytesIO(archive))\n",
    "    \n",
    "    # get the filetypes so we can account for reading them all\n",
    "    # get any sub folders\n",
    "    dirs = list(set([os.path.dirname(x) for x in archive.namelist()]))\n",
    "\n",
    "    # if there are subfolders check their extensions\n",
    "    extens = list(set([d.split('.')[-1] for d in dirs]))\n",
    "\n",
    "    # group folders by extension\n",
    "    egrps = {e:[d for d in dirs if e in d] for e in extens}\n",
    "\n",
    "    # get folders without extensions....\n",
    "    fgrps = [d for d in dirs if d.split('.')[-1] not in extens]\n",
    "    \n",
    "    # sort the namelist by folder\n",
    "    \n",
    "    # match extensions to folders (really just for gdb)\n",
    "    \n",
    "    # based on folder type, read the file(s) and write to folder\n",
    "    \n",
    "    print('/tOpened zip from remote source:', )\n",
    "    \n",
    "    def read_csv_from_zip():\n",
    "        # if its a csv file\n",
    "        \"\"\"\n",
    "        output\n",
    "            h3 encoded parquet file from polars\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # loop through names and save one at a time to save memory\n",
    "            for f in range(len(archive.namelist())):\n",
    "                print('\\t\\tReading file:', archive.namelist()[f])\n",
    "                pl_h3(pl.read_csv(archive.open(archive.namelist()[f]), \n",
    "                                  encoding=\"utf8-lossy\"), \n",
    "                      lat_col='LAT', \n",
    "                      lon_col='LON', \n",
    "                      new_col='H3', \n",
    "                      resolution=16).write_parquet(f\"{outpath}{os.sep}{archive.namelist()[f].replace('.csv','_h3.parquet')}\")\n",
    "            print('\\t\\t\\tRead csv', f)\n",
    "        except:\n",
    "            print('\\t\\t\\tFailed csv', f)\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def read_to_file(name, ext='csv'):\n",
    "        \n",
    "        print('read_to_file() | NAME:', name)\n",
    "\n",
    "        if ext == 'csv':\n",
    "            print('\\t\\tCSV read')\n",
    "            return print('!!!1', read_csv_from_zip())\n",
    "        elif ext == 'gdb':\n",
    "            print('\\t\\tGDB read')\n",
    "            return print('!!!2', read_gdb_from_zip())\n",
    "\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        print('egrps', egrps)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print('fgrps', fgrps)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for e in egrps:\n",
    "        try:\n",
    "            read_to_file(egrps[e], ext=e)\n",
    "            print(e, 'Succeeded')\n",
    "        except:\n",
    "            print(e, 'Failed')\n",
    "    for f in fgrps:\n",
    "        try:\n",
    "            read_to_file(fgrps[f], ext=f)\n",
    "            print(f, 'Succeeded')\n",
    "        except:\n",
    "            print(f, 'Failed')\n",
    "    return archive, egrps, fgrps, extens\n",
    "    \n",
    "    \n",
    "    # what if its something else?!?!\n",
    "    \n",
    "        \n",
    "#                 \"\"\" \n",
    "#                 limit the fields if you want. not too big though so who cares\n",
    "#                 ['MMSI','BaseDateTime',\n",
    "#                 'LAT', 'LON','SOG','COG',\n",
    "#                 'Heading', 'VesselName', \n",
    "#                 'IMO', 'CallSign', \n",
    "#                 'VesselType', 'Status',\n",
    "#                 'Length','Width','Draft',\n",
    "#                 'Cargo', 'TransceiverClass']\n",
    "#                 \"\"\"\n",
    "            \n",
    "    return archive, egrps, fgrps\n",
    "\n",
    "# here is the test run\n",
    "archive, egrps, fgrps, extens = save_dat(listFD(url.replace('YEAR', str(2009)), ext='zip')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d8092b",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc476d25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files from: https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2009/\n",
      "/tOpened zip from remote source:\n",
      "egrps {'gdb': ['Zone10_2009_01.gdb']}\n",
      "fgrps []\n",
      "read_to_file() | NAME: gdb\n",
      "\t\tCSV read\n",
      "\t\tReading file: Zone10_2009_01.gdb/a00000001.gdbindexes\n",
      "\t\t\tFailed csv 0\n",
      "!!!1 False\n",
      "gdb Succeeded\n",
      "\t\t 2009 \n",
      "\t\t https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2009//01_January_2009/Zone10_2009_01.zip Failed\n",
      "Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Polars found a filename. Ensure you pass a path to the file instead of a python file object when possible for best performance."
     ]
    }
   ],
   "source": [
    "# download AIS data from https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2022/\n",
    "\n",
    "for i in range(2009, 2022): # these are the available years, 2009-2022\n",
    "    \n",
    "    # read file names\n",
    "    url = f'https://coast.noaa.gov/htdata/CMSP/AISDataHandler/{i}/'\n",
    "    \n",
    "    print(f\"Reading files from:\", url)\n",
    "    \n",
    "    urls = listFD(url, ext='zip') # this gets the zip files\n",
    "\n",
    "    # get the data\n",
    "    for u in urls:\n",
    "        try:\n",
    "            df = pl_h3(pl.from_pandas(save_dat(u)), \n",
    "                       lat_col='LAT', \n",
    "                       lon_col='LON', \n",
    "                       new_col='H3', \n",
    "                       resolution=16)\n",
    "            \n",
    "            print('\\t\\t',i,'\\n\\t\\t',u, 'Success')\n",
    "            \n",
    "            df.write_parquet(filename)\n",
    "            \n",
    "        except:\n",
    "            print('\\t\\t',i,'\\n\\t\\t', u, 'Failed')\n",
    "        \n",
    "        print('Test')\n",
    "        break\n",
    "\n",
    "    # log/store our own minimized and enriched\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d89275dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tOpened zip from remote source:\n",
      "\t\tReading file: Zone10_2009_01.gdb/a00000001.gdbindexes\n",
      "\t\t\tFailed csv 0\n",
      "False\n",
      "gdb Succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Polars found a filename. Ensure you pass a path to the file instead of a python file object when possible for best performance."
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<zipfile.ZipFile file=<_io.BytesIO object at 0x7fef118efd10> mode='r'>,\n",
       " {'gdb': ['Zone10_2009_01.gdb']},\n",
       " [],\n",
       " ['gdb'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = save_dat(u)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2489b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to read folders by extension\n",
    "for ex in extens:\n",
    "    if ex in egrps.keys():\n",
    "        print(True)\n",
    "        pass\n",
    "    elif ex in fgrps.keys():\n",
    "        print(True)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a947f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Register the streaming http handlers with urllib2\n",
    "# register_openers()\n",
    "\n",
    "# # Use multipart encoding for the input files\n",
    "# datagen, headers = multipart_encode({ 'files[]': open('Zone10_2009_01.gdb', 'rb')})\n",
    "\n",
    "# datagen, headers = multipart_encode({ 'files[]': open(u, 'rb')})\n",
    "\n",
    "# # Create the request object\n",
    "# request = urllib2.Request(u, datagen, headers)\n",
    "\n",
    "# # Do the request and get the response\n",
    "# # Here the GDB file gets converted to CSV\n",
    "# response = urllib2.urlopen(request)\n",
    "\n",
    "# # Check if an error came back\n",
    "# if response.info().getheader('Content-Type') == 'application/json':\n",
    "#     print response.read()\n",
    "#     sys.exit(1)\n",
    "\n",
    "# # Write the response to /tmp/output.zip\n",
    "# with open('/tmp/output.zip', 'wb') as local_file:\n",
    "#     local_file.write(response.read())\n",
    "\n",
    "# print 'Conversion result successfully written to /tmp/output.zip!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca8cf29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf = gpd.read_file(gdb_file, \n",
    "#                   rows=slice(int(start), int(stop-1)), \n",
    "#                   engine='fiona',\n",
    "#                   layer=layer)\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47464a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_h3(pl.from_numpy(gdf.to_numpy(), \n",
    "                    columns=list(gdf.columns)), # this is atrocious, triple conversion, but cant be helped til geopolars and geoparquet work\n",
    "      lat_col='LAT', \n",
    "      lon_col='LON', \n",
    "      new_col='H3', \n",
    "      resolution=16).write_parquet(f\"{outpath}{os.sep}{archive.namelist()[f].replace('.csv','_h3.parquet')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadbd931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow as pa\n",
    "\n",
    "# import geopolars as gpl\n",
    "\n",
    "# reader = pa.ipc.open_file(gdb_file)\n",
    "# table = reader.read_all()\n",
    "\n",
    "# df = gpl.from_arrow(table)\n",
    "# geom = df.get_column(\"geometry\")\n",
    "# out = geom.centroid()\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "728fe10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\data\\\\AIS\\\\global AIS from NOAA\\\\Zone10_2009_01\\\\Zone10_2009_01.gdb'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdb_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0555d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
