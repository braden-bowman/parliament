{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a88638",
   "metadata": {},
   "source": [
    "# make an AOI detector/drawer in h3\n",
    "\"\"\"\n",
    "1. ETL - polars\n",
    "    convert datetime from string to epoch\n",
    "    convert lat lon to h3\n",
    "        resolution - \n",
    "    \n",
    "2. Enrichment\n",
    "    POI table - \n",
    "        decide who is of interest, maybe select some sketchy ports and use those to tag\n",
    "        can we get vessel nationalitys? is there a vessel registration API????\n",
    "        \n",
    "3. Scoring - RFM style\n",
    "    to dupe redline - port polygons - World Port Index\n",
    "        https://msi.nga.mil/Publications/WPI     ### THIS HAS THE FILE USED BELOW\n",
    "        \n",
    "        POLYGONS / shape files - REST API\n",
    "            https://www.marineregions.org/downloads.php\n",
    "            \n",
    "        https://data.humdata.org/dataset/global-ports\n",
    "        https://hub.arcgis.com/datasets/EDT::world-port-index/explore?location=0.017789%2C1.000000%2C1.88\n",
    "        https://datacatalog.worldbank.org/search/dataset/0060145\n",
    "        \n",
    "        portsheds?\n",
    "            https://datacatalog.worldbank.org/search/dataset/0060145\n",
    "        \n",
    "        \n",
    "        open street map?\n",
    "        \n",
    "\n",
    "3. Counts for naive bayes\n",
    "        total per hex\n",
    "        POI per hex\n",
    "        \n",
    "4. Cluster hex's\n",
    "\n",
    "5. Merge cluster groups into polygons\n",
    "\n",
    "6. Match polygons to ports\n",
    "\n",
    "7. Make list of 'nefarious' ports\n",
    "    label intersecting polygons\n",
    "    \n",
    "8. use scan_csv to make hex probability assignments\n",
    "    use logging to retain calculations from previous files so new #'s are updates\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c678526-bec7-435b-8000-1802d0d3d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install folium geohash "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcef5937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "\"\"\"\n",
    "https://pola-rs.github.io/polars/py-polars/html/reference/expressions/index.html\n",
    "https://pola-rs.github.io/polars-book/user-guide/introduction.html\n",
    "\"\"\"\n",
    "\n",
    "import geopandas as gpd\n",
    "# import geopolars as gpl\n",
    "\"\"\"\n",
    "https://geopolars.org/\n",
    "\"\"\"\n",
    "\n",
    "import folium\n",
    "\n",
    "import shapely\n",
    "# import geohash\n",
    "\n",
    "from shapely.geometry import Polygon, Point\n",
    "import shapely.wkt\n",
    "\n",
    "import os\n",
    "\n",
    "import math\n",
    "import h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47dd3d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some globals\n",
    "TESTING = False\n",
    "ETL_PATH = r\"'/mnt/d/data/AIS/ais_csv/h3_processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a3cf962-4335-462f-907c-75454d667211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/Asus/Code/parliament/notebooks'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba94ca21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Port files breakdown\n",
    "port = pd.read_csv('/mnt/d/data/Ports/UpdatedPub150.csv')\n",
    "port = pl.from_pandas(port)\n",
    "\n",
    "# drop most the columns. could be interesting for things though....\n",
    "port = port[['World Port Index Number', 'Region Name', \n",
    "      'Main Port Name', 'Alternate Port Name', \n",
    "      'UN/LOCODE', 'Country Code', 'World Water Body', \n",
    "#       'IHO S-130 Sea Area', 'Sailing Direction or Publication', \n",
    "#       'Publication Link', 'Standard Nautical Chart', \n",
    "#       'IHO S-57 Electronic Navigational Chart', \n",
    "#       'IHO S-101 Electronic Navigational Chart', \n",
    "#       'Digital Nautical Chart', 'Tidal Range (m)', \n",
    "#       'Entrance Width (m)', 'Channel Depth (m)', \n",
    "#       'Anchorage Depth (m)', 'Cargo Pier Depth (m)', \n",
    "#       'Oil Terminal Depth (m)', 'Liquified Natural Gas Terminal Depth (m)', \n",
    "      'Maximum Vessel Length (m)', 'Maximum Vessel Beam (m)', \n",
    "      'Maximum Vessel Draft (m)', 'Offshore Maximum Vessel Length (m)', \n",
    "      'Offshore Maximum Vessel Beam (m)', 'Offshore Maximum Vessel Draft (m)', \n",
    "      'Harbor Size', 'Harbor Type', 'Harbor Use', \n",
    "#       'Shelter Afforded', \n",
    "#       'Entrance Restriction - Tide', 'Entrance Restriction - Heavy Swell', \n",
    "#       'Entrance Restriction - Ice', 'Entrance Restriction - Other', \n",
    "#       'Overhead Limits', 'Underkeel Clearance Management System', \n",
    "      'Good Holding Ground', 'Turning Area', \n",
    "      'Port Security', \n",
    "#       'Estimated Time of Arrival Message', 'Quarantine - Pratique', \n",
    "#       'Quarantine - Sanitation', 'Quarantine - Other', \n",
    "#       'Traffic Separation Scheme', 'Vessel Traffic Service', \n",
    "      'First Port of Entry', \n",
    "#       'US Representative', 'Pilotage - Compulsory', \n",
    "#       'Pilotage - Available', 'Pilotage - Local Assistance', 'Pilotage - Advisable', \n",
    "#       'Tugs - Salvage', 'Tugs - Assistance', 'Communications - Telephone', \n",
    "#       'Communications - Telefax', 'Communications - Radio', \n",
    "#       'Communications - Radiotelephone', 'Communications - Airport', \n",
    "#       'Communications - Rail', 'Search and Rescue', 'NAVAREA', \n",
    "#       'Facilities - Wharves', 'Facilities - Anchorage', \n",
    "#       'Facilities - Dangerous Cargo Anchorage', 'Facilities - Med Mooring', \n",
    "#       'Facilities - Beach Mooring', 'Facilities - Ice Mooring', \n",
    "#       'Facilities - Ro-Ro', 'Facilities - Solid Bulk', 'Facilities - Liquid Bulk', \n",
    "#       'Facilities - Container', 'Facilities - Breakbulk', \n",
    "#       'Facilities - Oil Terminal', 'Facilities - LNG Terminal', \n",
    "#       'Facilities - Other', 'Medical Facilities', 'Garbage Disposal', \n",
    "#       'Chemical Holding Tank Disposal', 'Degaussing', \n",
    "#       'Dirty Ballast Disposal', 'Cranes - Fixed', \n",
    "#       'Cranes - Mobile', 'Cranes - Floating', \n",
    "#       'Cranes - Container', 'Lifts - 100+ Tons', \n",
    "#       'Lifts - 50-100 Tons', 'Lifts - 25-49 Tons', \n",
    "#       'Lifts - 0-24 Tons', 'Services - Longshoremen', \n",
    "#       'Services - Electricity', 'Services - Steam', \n",
    "#       'Services - Navigation Equipment', 'Services - Electrical Repair', \n",
    "#       'Services - Ice Breaking', 'Services - Diving', \n",
    "#       'Supplies - Provisions', 'Supplies - Potable Water', \n",
    "#       'Supplies - Fuel Oil', 'Supplies - Diesel Oil', 'Supplies - Aviation Fuel', \n",
    "#       'Supplies - Deck', 'Supplies - Engine', \n",
    "      'Repairs', \n",
    "      'Dry Dock', 'Railway', \n",
    "      'Latitude', 'Longitude']]\n",
    "\n",
    "# sorted(port['Country Code'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd1c8e97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>Postal</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lng</th>\n",
       "      <th>Type</th>\n",
       "      <th>ISO3</th>\n",
       "      <th>REGION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>Mongla (Chalna),Bangladesh</td>\n",
       "      <td>22.488900</td>\n",
       "      <td>89.595800</td>\n",
       "      <td>Bulk</td>\n",
       "      <td>BGD</td>\n",
       "      <td>South Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>Suape (Recife),Brazil</td>\n",
       "      <td>-8.055556</td>\n",
       "      <td>-34.891111</td>\n",
       "      <td>General</td>\n",
       "      <td>BRA</td>\n",
       "      <td>Latin America &amp; Caribbean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>PecÎ˜m (near Fortaleza),Brazil</td>\n",
       "      <td>-3.533611</td>\n",
       "      <td>-38.785833</td>\n",
       "      <td>General</td>\n",
       "      <td>BRA</td>\n",
       "      <td>Latin America &amp; Caribbean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>Ponta da Madeira,Brazil</td>\n",
       "      <td>-2.565000</td>\n",
       "      <td>-44.370000</td>\n",
       "      <td>Bulk</td>\n",
       "      <td>BRA</td>\n",
       "      <td>Latin America &amp; Caribbean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>Ponta do Ubu,Brazil</td>\n",
       "      <td>-20.788500</td>\n",
       "      <td>-40.573700</td>\n",
       "      <td>Bulk</td>\n",
       "      <td>BRA</td>\n",
       "      <td>Latin America &amp; Caribbean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>134</td>\n",
       "      <td>Surigao,Philippines</td>\n",
       "      <td>9.789000</td>\n",
       "      <td>125.495000</td>\n",
       "      <td>Bulk</td>\n",
       "      <td>PHL</td>\n",
       "      <td>East Asia &amp; Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>130</td>\n",
       "      <td>Davao,Philippines</td>\n",
       "      <td>7.073060</td>\n",
       "      <td>125.612780</td>\n",
       "      <td>General</td>\n",
       "      <td>PHL</td>\n",
       "      <td>East Asia &amp; Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>121</td>\n",
       "      <td>Port Moresby ,Papua New Guinea</td>\n",
       "      <td>-9.477230</td>\n",
       "      <td>147.150890</td>\n",
       "      <td>General</td>\n",
       "      <td>PNG</td>\n",
       "      <td>East Asia &amp; Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>176</td>\n",
       "      <td>Lamu Mainland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General</td>\n",
       "      <td>KEN</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>177</td>\n",
       "      <td>Port Sudan, Sudan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General</td>\n",
       "      <td>SDN</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     OBJECTID                          Postal        Lat         Lng     Type  \\\n",
       "0          12      Mongla (Chalna),Bangladesh  22.488900   89.595800     Bulk   \n",
       "1          19           Suape (Recife),Brazil  -8.055556  -34.891111  General   \n",
       "2          20   PecÎ˜m (near Fortaleza),Brazil  -3.533611  -38.785833  General   \n",
       "3          23         Ponta da Madeira,Brazil  -2.565000  -44.370000     Bulk   \n",
       "4          24             Ponta do Ubu,Brazil -20.788500  -40.573700     Bulk   \n",
       "..        ...                             ...        ...         ...      ...   \n",
       "172       134             Surigao,Philippines   9.789000  125.495000     Bulk   \n",
       "173       130               Davao,Philippines   7.073060  125.612780  General   \n",
       "174       121  Port Moresby ,Papua New Guinea  -9.477230  147.150890  General   \n",
       "175       176                   Lamu Mainland        NaN         NaN  General   \n",
       "176       177               Port Sudan, Sudan        NaN         NaN  General   \n",
       "\n",
       "    ISO3                     REGION  \n",
       "0    BGD                 South Asia  \n",
       "1    BRA  Latin America & Caribbean  \n",
       "2    BRA  Latin America & Caribbean  \n",
       "3    BRA  Latin America & Caribbean  \n",
       "4    BRA  Latin America & Caribbean  \n",
       "..   ...                        ...  \n",
       "172  PHL        East Asia & Pacific  \n",
       "173  PHL        East Asia & Pacific  \n",
       "174  PNG        East Asia & Pacific  \n",
       "175  KEN         Sub-Saharan Africa  \n",
       "176  SDN         Sub-Saharan Africa  \n",
       "\n",
       "[177 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# portsheds???? what are these?\n",
    "df = pd.read_csv('/mnt/d/data/Ports/major_ports.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b34fed3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolution 1\t 511201962310545 m\n",
      "Resolution 2\t 73028851758649 m\n",
      "Resolution 3\t 10432693108378 m\n",
      "Resolution 4\t 1490384729768 m\n",
      "Resolution 5\t 212912104253 m\n",
      "Resolution 6\t 30416014893 m\n",
      "Resolution 7\t 4345144985 m\n",
      "Resolution 8\t 620734998 m\n",
      "Resolution 9\t 88676428 m\n",
      "Resolution 10\t 12668061 m\n",
      "Resolution 11\t 1809723 m\n",
      "Resolution 12\t 258532 m\n",
      "Resolution 13\t 36933 m\n",
      "Resolution 14\t 5276 m\n",
      "Resolution 15\t 754 m\n",
      "Resolution 16\t 108 m\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "about h3\n",
    "    https://www.uber.com/blog/h3/\n",
    "    \n",
    "resolutions... 122 hex's in base layer, each layer down is 1/7th the area per, 16 layers\n",
    "sphere SA=4Ï€r^2\n",
    "radius earth = 6378.1 kilometers\n",
    "\"\"\"\n",
    "\n",
    "# this finds hex area by resolution level\n",
    "re = 6378100\n",
    "sae = 4*math.pi *(re**2)\n",
    "\n",
    "print(\"Resolution 1\\t\", round(sae), 'm')\n",
    "\n",
    "for a in range(1,16):\n",
    "    sae = sae/7\n",
    "    print(f\"Resolution {a+1}\\t\", round(sae), 'm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a10a5c",
   "metadata": {},
   "source": [
    "# AIS in H3 Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d46979",
   "metadata": {},
   "source": [
    "## AIS\n",
    "### its what we have, i use some csvs i had but i think the api is open\n",
    " \n",
    "\"\"\"\n",
    "what is AIS? looks like some interesting papers @ the end\n",
    "    https://en.wikipedia.org/wiki/Automatic_identification_system\n",
    "    \n",
    "free sources\n",
    "\n",
    "    EXCELLENT article - plys how to build a receiver. I like this\n",
    "    https://xyzt.ai/2020/07/09/where-to-obtain-ais-data-and-easily-analyze-it-without-writing-a-single-line-of-code/#:~:text=A%20great%20source%20of%20free,published%20on%20an%20FTP%20site.\n",
    "    \n",
    "    global api\n",
    "    https://aisstream.io/\n",
    "    \n",
    "    us waters only in csv's\n",
    "    https://marinecadastre.gov/ais/\n",
    "    \n",
    "    norwegian api\n",
    "    https://kystverket.no/navigasjonstjenester/ais/tilgang-pa-ais-data/\n",
    "    \n",
    "paid sources (free trials)\n",
    "    https://www.marinetraffic.com/en/ais-api-services\n",
    "\n",
    "\n",
    "discussion of the columns - different source though\n",
    "    https://api.vtexplorer.com/docs/response-ais.html\n",
    "    \n",
    "mmsi / id lookup\n",
    "    https://wireless2.fcc.gov/UlsApp/UlsSearch/searchShip.jsp\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f541b3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'ais_csv',\n",
       " 'bb_ec50edf2-e301-4dc8-8422-af0425265dfd.csv',\n",
       " 'bb_f77084a2-b037-4d18-ba4c-f85f0120f99a.csv',\n",
       " 'data_webmercator_coords',\n",
       " 'From where',\n",
       " 'global AIS from NOAA',\n",
       " 'NOAA FTP download',\n",
       " 'spatial temporal clustering AIS with BB.ipynb',\n",
       " 'Untitled.ipynb']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # free global API\n",
    "# # https://aisstream.io/documentation\n",
    "# # my api key  - \n",
    "# apikey = 'b4871d51295ea8305148164f169d8536bc302cdf'\n",
    "# # https://leonrichter.de/posts/pyais/\n",
    "# # https://pypi.org/project/libais/\n",
    "\n",
    "# # from setuptools import setup\n",
    "# # setup(\n",
    "# #     name='aisstream.io-example-simple',\n",
    "# #     author='aisstream.io',\n",
    "# #     version='0.1',\n",
    "# #     install_requires=[\n",
    "# #         'websockets',\n",
    "# #         'asyncio'\n",
    "# #     ],\n",
    "# # )\n",
    "# import asyncio\n",
    "# import websockets\n",
    "# import json\n",
    "# from datetime import datetime, timezone\n",
    "\n",
    "# def async def connect_ais_stream():\n",
    "\n",
    "#     async with websockets.connect(\"wss://stream.aisstream.io/v0/stream\") as websocket:\n",
    "#         subscribe_message = {\"APIKey\": \"<YOUR API KEY>\", \"BoundingBoxes\": [[[-180, -90], [180, 90]]]}\n",
    "\n",
    "#         subscribe_message_json = json.dumps(subscribe_message)\n",
    "#         await websocket.send(subscribe_message_json)\n",
    "\n",
    "#         async for message_json in websocket:\n",
    "#             message = json.loads(message_json)\n",
    "#             message_type = message[\"MessageType\"]\n",
    "\n",
    "#             if message_type == \"PositionReport\":\n",
    "#                 # the message parameter contains a key of the message type which contains the message itself\n",
    "#                 ais_message = message['Message']['PositionReport']\n",
    "#                 print(f\"[{datetime.now(timezone.utc)}] ShipId: {ais_message['UserID']} Latitude: {ais_message['Latitude']} Longitude: {ais_message['Longitude']}\")\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# asyncio.run(connect_ais_stream())\n",
    "    \n",
    "os.listdir('/mnt/d/data/AIS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b26ab621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIS_2021_03_28.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory (os error 2): /mnt/d/data/AIS/ais_csv\\AIS_2021_03_28.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# read in a whole day/file\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(files[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m pf \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mais_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# datetime doesnt warrent a function\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# https://pola-rs.github.io/polars/py-polars/html/reference/series/timeseries.html\u001b[39;00m\n\u001b[1;32m     11\u001b[0m pf \u001b[38;5;241m=\u001b[39m pf\u001b[38;5;241m.\u001b[39mwith_column(pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaseDateTime\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrptime(pl\u001b[38;5;241m.\u001b[39mDatetime, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/polars/_utils/deprecation.py:134\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    131\u001b[0m     _rename_keyword_argument(\n\u001b[1;32m    132\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, version\n\u001b[1;32m    133\u001b[0m     )\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/polars/_utils/deprecation.py:134\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    131\u001b[0m     _rename_keyword_argument(\n\u001b[1;32m    132\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, version\n\u001b[1;32m    133\u001b[0m     )\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/polars/_utils/deprecation.py:134\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    131\u001b[0m     _rename_keyword_argument(\n\u001b[1;32m    132\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, version\n\u001b[1;32m    133\u001b[0m     )\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/polars/io/csv/functions.py:397\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(source, has_header, columns, new_columns, separator, comment_prefix, quote_char, skip_rows, dtypes, schema, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, use_pyarrow, storage_options, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines)\u001b[0m\n\u001b[1;32m    385\u001b[0m         dtypes \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    386\u001b[0m             new_to_current\u001b[38;5;241m.\u001b[39mget(column_name, column_name): column_dtype\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m column_name, column_dtype \u001b[38;5;129;01min\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    388\u001b[0m         }\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _prepare_file_arg(\n\u001b[1;32m    391\u001b[0m     source,\n\u001b[1;32m    392\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    395\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    396\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m data:\n\u001b[0;32m--> 397\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprojection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseparator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquote_char\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquote_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnull_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnull_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtry_parse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_parse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_schema_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_schema_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8-lossy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrechunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrechunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow_index_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_index_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow_index_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_index_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43meol_char\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meol_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraise_if_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_if_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_columns:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _update_columns(df, new_columns)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/polars/dataframe/frame.py:655\u001b[0m, in \u001b[0;36mDataFrame._read_csv\u001b[0;34m(cls, source, has_header, columns, separator, comment_prefix, quote_char, skip_rows, dtypes, schema, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines)\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    653\u001b[0m projection, columns \u001b[38;5;241m=\u001b[39m handle_projection_columns(columns)\n\u001b[0;32m--> 655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m=\u001b[39m \u001b[43mPyDataFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_schema_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprojection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrechunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquote_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessed_null_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_parse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_prepare_row_index_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_index_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_index_offset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43meol_char\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meol_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraise_if_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_if_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory (os error 2): /mnt/d/data/AIS/ais_csv\\AIS_2021_03_28.csv"
     ]
    }
   ],
   "source": [
    "ais_path = '/mnt/d/data/AIS/ais_csv'\n",
    "files = [i for i in os.listdir(ais_path) if '.csv' in i]\n",
    "\n",
    "# read in a whole day/file\n",
    "print(files[-1])\n",
    "pf = pl.read_csv(ais_path+'\\\\'+files[-1])\n",
    "\n",
    "\n",
    "\n",
    "# datetime doesnt warrent a function\n",
    "# https://pola-rs.github.io/polars/py-polars/html/reference/series/timeseries.html\n",
    "pf = pf.with_column(pl.col('BaseDateTime').str.strptime(pl.Datetime, \"%Y-%m-%dT%H:%M:%S\"))\n",
    "pf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e2237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add H3 feature and put timestamps in datetime format\n",
    "def pl_h3(pf, lat_col='LAT', lon_col='LON', new_col='H3', resolution=16):\n",
    "    \"\"\"\n",
    "    ### TODO: write a preprocessor function that uses this and re-saves the files\n",
    "    \n",
    "    this converts lat lon to h3 in polars\n",
    "    (function to apply a function to 2 columns in polars....)\n",
    "    \"\"\"\n",
    "    return pf.with_columns(pl.struct([lon_col,lat_col]).apply(lambda x: h3.geo_to_h3(lat=x[lat_col], lng=x[lon_col], resolution=7)).alias(new_col))\n",
    "\n",
    "\n",
    "pf = pl_h3(pf, lat_col='LAT', lon_col='LON', new_col='H3', resolution=16)\n",
    "pf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfce1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add H3 feature to ports - resolution of 10 chosen doing visual comparison with vessels that seem to have gone there\n",
    "port = pl_h3(port, lat_col='Latitude', lon_col='Longitude', new_col='H3', resolution=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a7d0a",
   "metadata": {},
   "source": [
    "### These borrowed plotting functions do clusters and dont account for hex resolution..... fix it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36843edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting modified from https://github.com/uber/h3-py-notebooks/blob/master/notebooks/usage.ipynb\n",
    "zoom = 10\n",
    "def visualize_hexagons(hexagons, color=\"red\", folium_map=None):\n",
    "    \"\"\"\n",
    "    hexagons is a list of hexcluster. Each hexcluster is a list of hexagons. \n",
    "    eg. [[hex1, hex2], [hex3, hex4]]\n",
    "    \"\"\"\n",
    "    polylines = []\n",
    "    lat = []\n",
    "    lng = []\n",
    "    for hex in hexagons:\n",
    "        polygons = h3.h3_set_to_multi_polygon([hex], geo_json=False)\n",
    "        # flatten polygons into loops.\n",
    "        outlines = [loop for polygon in polygons for loop in polygon]\n",
    "        polyline = [outline + [outline[0]] for outline in outlines][0]\n",
    "        lat.extend(map(lambda v:v[0],polyline))\n",
    "        lng.extend(map(lambda v:v[1],polyline))\n",
    "        polylines.append(polyline)\n",
    "    \n",
    "    if folium_map is None:\n",
    "        m = folium.Map(location=[sum(lat)/len(lat), sum(lng)/len(lng)], zoom_start=zoom, tiles='cartodbpositron')\n",
    "    else:\n",
    "        m = folium_map\n",
    "    for polyline in polylines:\n",
    "        my_PolyLine=folium.PolyLine(locations=polyline,weight=8,color=color)\n",
    "        m.add_child(my_PolyLine)\n",
    "    return m\n",
    "    \n",
    "\n",
    "def visualize_polygon(polyline, color):\n",
    "    polyline.append(polyline[0])\n",
    "    lat = [p[0] for p in polyline]\n",
    "    lng = [p[1] for p in polyline]\n",
    "    m = folium.Map(location=[sum(lat)/len(lat), sum(lng)/len(lng)], zoom_start=zoom, tiles='cartodbpositron')\n",
    "    my_PolyLine=folium.PolyLine(locations=polyline,weight=8,color=color)\n",
    "    m.add_child(my_PolyLine)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af4bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick an individual vessel to start with a case study and write out the functionality\n",
    "mmsi = 316003664\n",
    "pf['MMSI'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f72c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h3_address = h3.geo_to_h3(37.3615593, -122.0553238, 9) # lat, lng, hex resolution\n",
    "vessel = pf.filter(pl.col('MMSI') == mmsi)\n",
    "vessel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c478765",
   "metadata": {},
   "source": [
    "### Look at a single vessel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6dd406",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = visualize_hexagons(vessel['H3'].unique())\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132930ce",
   "metadata": {},
   "source": [
    "### I think if you fit a curve through the centroids youd find the path.... or just plot the lat/lon's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f2e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time around we dont want to use python dictrionaries to store!!!\n",
    "# to make it faster we are going to use polars datastructures to count and calc by hex bin\n",
    "\n",
    "# pandas value counts equivalent\n",
    "vessel.select(pl.col('H3')).n_unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e031389c",
   "metadata": {},
   "source": [
    "### All the ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8899241",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = visualize_hexagons(port['H3'].unique())\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fdf7b7",
   "metadata": {},
   "source": [
    "### counts per hex for a single vessel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c252fbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q = (\n",
    "    vessel.lazy()\n",
    "    .groupby(\"H3\")\n",
    "    .agg(\n",
    "        [\n",
    "            pl.max('BaseDateTime'),\n",
    "            pl.count(),\n",
    "            pl.n_unique('BaseDateTime').alias(\"BaseDateTime_count\"),\n",
    "            \n",
    "        ]\n",
    "    )\n",
    "    .sort(\"count\", reverse=True)\n",
    ")\n",
    "\n",
    "q = q.collect()\n",
    "q.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf027f2",
   "metadata": {},
   "source": [
    "### What ports was the vessl seen at????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd0fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "visited = pl.DataFrame(port['H3']).join(pl.DataFrame(vessel['H3']),\n",
    "                                        on=\"H3\",\n",
    "                                        how='inner').unique()\n",
    "visited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f96f9f3",
   "metadata": {},
   "source": [
    "## Based on Visual Inspection....\n",
    "\n",
    "### I think we should be making the vessel intersection be within 1 hex, rather than the exact hex\n",
    "\n",
    "If we do so then the result becomes as follows.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2251d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add neighbors to ports - easier - name them same as port - or harder - do an aggregation\n",
    "buff_ports = port['Main Port Name',  'H3', 'Country Code', 'World Water Body', 'Longitude', 'Latitude']\n",
    "\n",
    "\n",
    "\n",
    "len('872a1061bffffff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2774398",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridDistance = 2\n",
    "\n",
    "#test\n",
    "cell = buff_ports['H3'][0]\n",
    "print('Hex in:\\n\\n', cell, '\\n\\nNeighbors out:')\n",
    "# h3.hex_ring(cell, gridDistance) # unordered set of cells with H3 distance, so 1 == one cell away\n",
    "h3.k_ring(cell, gridDistance) # k_ring is allegedly faster but slightly less accurate, but i like that it includes the originial cell...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac9650",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply by row (selection context) - find all neighbors, unpack neighbors and give same label....\n",
    "neigbor_ports = buff_ports[ 'H3', \n",
    "                           'Main Port Name', \n",
    "                           'Country Code',\n",
    "                          ].with_columns(pl.col(\"H3\").apply(lambda cell: list(h3.k_ring(cell, \n",
    "                                                                                        gridDistance))).alias(\"h3\")).explode('h3')\n",
    "neigbor_ports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27752930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find new intersections with the vessel using neighbors - h3 column\n",
    "neighbors_visited = neigbor_ports.join(pl.DataFrame(vessel['H3']),\n",
    "                                       left_on='h3',\n",
    "                                       right_on=\"H3\",\n",
    "                                       how='inner').unique()\n",
    "neighbors_visited['Main Port Name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e99e15",
   "metadata": {},
   "source": [
    "###  Now for RFM......\n",
    "https://www.geeksforgeeks.org/rfm-analysis-analysis-using-python/\n",
    "\n",
    "Select our ports\n",
    "\n",
    "Scan files to get rfm scores by vessel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# designate ports\n",
    "aois = port.filter(pl.col(\"H3\").is_in(neighbors_visited[\"H3\"].to_list()))['Main Port Name'].to_list()\n",
    "aois"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fbd69b",
   "metadata": {},
   "source": [
    "### This reads through every file. pick and chose what to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53874c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # will probably be better to scan the csv's in the long run so we can handle all the data.... here is how\n",
    "# results = {}\n",
    "\n",
    "# to_read = 2 # number of files to read, -1 goes to the end\n",
    "\n",
    "# # this gets all the data for a vessel from all files\n",
    "# for i in range(len(files[:to_read])):\n",
    "#     s = (pl.scan_csv(ais_path+'\\\\'+files[i])\n",
    "#         .filter(pl.col(\"MMSI\") == 316044349) # select all in the file with matching mmsi\n",
    "#         .groupby(\"BaseDateTime\") # gets only one obs per value in this column (timestamp basically returns all), to use H3 we have to resave all the files....\n",
    "#         .agg(pl.all().sum())\n",
    "#         ) # and this would find one instance of every hex it was in, if we groupby H3\n",
    "#     results[files[i]] = s.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af6e259",
   "metadata": {},
   "source": [
    "### Get ALL the vessel id's to use while we get scores (so we know whats missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405deff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test getting all the vessel numbers out of the files\n",
    "mmsi = pl.concat(\n",
    "    [pl.scan_csv(ais_path+'\\\\'+files[i]).select(pl.col(\"MMSI\").unique()).collect() for i in range(len(files))]\n",
    ")\n",
    "mmsi = mmsi['MMSI'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bb0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "so should we scan by....\n",
    "                        port\n",
    "                        hex\n",
    "                        hex cluster (for plotting)\n",
    "                            which is basically port neighbors\n",
    "                        vessel\n",
    "                    file - well we HAVE to scan by file, i suppose\n",
    "                    \n",
    "depends on the task.... each if better for something different, but we dont want to scan multiple times....\n",
    "        or do we???\n",
    "        by file is forced so lets do them all on the working query\n",
    "        gonna need logging, \n",
    "            we have to remember what we did and how much went into so we can adjust some stats as we go\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00697f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_time = datetime.now()\n",
    "start_time = start_time-timedelta(minutes=12) # test\n",
    "start_time-datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc0f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "completed\n",
    "outfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a93be3",
   "metadata": {},
   "source": [
    "### ETL AIS files with H3 encoding into parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeed365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this gets the base data we are using per vessel, from each individual file\n",
    "start_time = datetime.now()\n",
    "results={}\n",
    "to_read = -1\n",
    "\n",
    "tot = len(files)\n",
    "completed = os.listdir(ETL_PATH)\n",
    "\n",
    "for i in range(len(files[:to_read])):\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print('Performing H3 encoding and parquet encoding')\n",
    "    print(f\"\\tProcessing file {i+1}/{tot}:\\t {files[i]}\")\n",
    "    \n",
    "    \n",
    "    filename = files[i].replace('.csv', '_h3.parquet')    \n",
    "    outfile = f\"{ETL_PATH}{os.sep}{filename}\"\n",
    "    \n",
    "    if filename in completed:\n",
    "        continue\n",
    "    \n",
    "    s = pl.scan_csv(ais_path+'\\\\'+files[i]).select([\"MMSI\", \"BaseDateTime\", \"LAT\", \"LON\"])\n",
    "    \n",
    "    # encode h3.....\n",
    "    s = pl_h3(s, lat_col='LAT', lon_col='LON', new_col='H3', resolution=16)\n",
    "    \n",
    "    results[files[i]] = s.groupby(\"MMSI\").agg(\n",
    "        [\n",
    "            pl.max('BaseDateTime').alias('last_seen'),\n",
    "            pl.col('BaseDateTime'),\n",
    "            pl.col('H3'),\n",
    "            pl.col(\"LAT\"),\n",
    "            pl.col(\"LON\")\n",
    "        ])\n",
    "    \n",
    "    # too big still, so instead we will save these as files and scan them \n",
    "    # since we are re-saving, we might as well switch to parquet for efficiency\n",
    "\n",
    "\n",
    "    s.collect().write_parquet(outfile)\n",
    "    \n",
    "    if TESTING == True:\n",
    "        break\n",
    "completed = os.listdir(ETL_PATH)\n",
    "# i think this is still too big once we get ALL the files\n",
    "# results = pl.concat(list(results.values())).collect()\n",
    "\n",
    "# calculate elapsed time\n",
    "elapsed = start_time-datetime.now()\n",
    "elapsed = round(abs(elapsed.total_seconds()),1) # put into minutes at some point\n",
    "\n",
    "print('___________________________________________________\\nOperation took:', elapsed, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f7003b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now we can filter by mmsi and use some kind of explode to look at the lists of H3 encodings\n",
    "filename = f\"{ETL_PATH}{os.sep}{completed[0]}\"\n",
    "pl.read_parquet(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdbe4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq = pl.scan_parquet(filename).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae634a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pq.groupby('MMSI').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7be803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s.groupby(\"MMSI\").agg(\n",
    "    [\n",
    "        pl.max('BaseDateTime').alias('last_seen'),\n",
    "]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd5b170",
   "metadata": {},
   "source": [
    "#### do RFM calculations by vessel with groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa910768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "should make a historic score for these..... back propogate(not what i mean) and track - poi table\n",
    "\n",
    "recency can have an opposite, anti recency, or mean etc. rarity maybe (this is an odd port to visit)\n",
    "    maybe match this stuff up with news reporting?????\n",
    "    \n",
    "frequency\n",
    "    how often they are visiting\n",
    "    match with news reporting???\n",
    "\"\"\"\n",
    "\n",
    "recent_date = pf['BaseDateTime'].max()\n",
    "\n",
    "q = (\n",
    "    pf.lazy()\n",
    "    .groupby(\"MMSI\")\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col('H3').unique(),\n",
    "            \n",
    "            # figuring out how to apply a function to a column in a group.... almost there. Problem is | acc=pl.lit() | i think\n",
    "            # i suppose we could just do it afterward.................\n",
    "#             pl.fold(acc=pl.lit(0),\n",
    "#                     f=lambda acc, x: acc.total_seconds(),\n",
    "#                     exprs=(pl.max('BaseDateTime')-recent_date)\n",
    "#                    ).alias('recency_score'),\n",
    "            \n",
    "            \n",
    "            #recency\n",
    "            pl.max('BaseDateTime').alias('last_seen'), # recency\n",
    "            (pl.max('BaseDateTime')-recent_date).alias('recency_score'), # this gives the score but needs to be done across days.... this file is all today only\n",
    "            # agism\n",
    "            pl.min('BaseDateTime').alias('first_seen'), # anti-recency / age\n",
    "            # score\n",
    "            \n",
    "            # frequency - count of times at port.....\n",
    "            pl.count().alias('observations'),\n",
    "            # score\n",
    "            \n",
    "            \n",
    "            # std in these is a measure of how far they typically travel - within this file anyway\n",
    "#             pl.std('LAT').alias('lat_std'),\n",
    "#             pl.std('LON').alias('lon_std'),\n",
    "            \n",
    "            # M, value - need an equation for this, maybe combo of home port, country of registration, # other aois visited\n",
    "            \n",
    "            \n",
    "            # gather vessel name, class etc. more than one means changes. why change??? youre suspect\n",
    "#             pl.col('CallSign').n_unique(),\n",
    "#             pl.col('VesselName').n_unique(),\n",
    "#             pl.col('VesselType').n_unique(),\n",
    "#             pl.col('TranscieverClass').n_unique(),\n",
    "#             pl.col('IMO').n_unique(),\n",
    "#             pl.col('Length').n_unique(),\n",
    "#             pl.col('Width').n_unique(),\n",
    "#             pl.col('Draft').n_unique(),\n",
    "            \n",
    "            \n",
    "            # get all the values theyve been seen as - look them up, maybe a pattern? or many is weird? idk\n",
    "#             pl.col('Status').unique(),\n",
    "#             pl.col('Cargo').unique(), # we may be able to match this against ports as those list product types\n",
    "            # ie, are you bringing the normal thing? how many things do you transport? just one?\n",
    "        ]\n",
    "    )\n",
    ").sort(\"recency_score\", reverse=False)\n",
    "\n",
    "j = q.collect()\n",
    "j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d3b98c",
   "metadata": {},
   "source": [
    "## Scan files to find counts by hex by timeperiod\n",
    "\n",
    "### parameters\n",
    "hex resolution - \n",
    "timeperiod - try day and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d197080c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37320458",
   "metadata": {},
   "source": [
    "## Scan files to find PORT VISITS by hex by timeperiod\n",
    "\n",
    "### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6788e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24b97bcb",
   "metadata": {},
   "source": [
    "### get vessel port path\n",
    "\n",
    "## using DAG's? or other graph techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b6c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "474fa63f",
   "metadata": {},
   "source": [
    "### nefarious paths - combinations....\n",
    "\n",
    "not just one to one, here to there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7057c6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcf32fa7",
   "metadata": {},
   "source": [
    "### find path combinations from origin x to destination y\n",
    "\n",
    "ie. going from tehran to crimea given some start and end windows, what paths exist?\n",
    "\n",
    "parameters\n",
    "    start and end window sizes\n",
    "    load/unload window time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfbde78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bb6eb2c",
   "metadata": {},
   "source": [
    "### any graph based AI/ML algs that can help us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e651de3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42be5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open street map and other location info\n",
    "\"\"\"\n",
    "\n",
    "https://date.nager.at/\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d7b276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpsd - drones, android map service, computers, robot submarines, and driverless cars.\n",
    "# ncreasingly common in recent generations of manned aircraft, marine navigation systems, and military vehicles\n",
    "\"\"\"\n",
    "https://gpsd.gitlab.io/gpsd/index.html\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e3da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight tracker data\n",
    "\"\"\"\n",
    "free\n",
    "    https://opensky-network.org/\n",
    "\n",
    "some free\n",
    "    https://aviation-edge.com/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c8d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wifi data\n",
    "\"\"\"\n",
    "some free\n",
    "    https://wigle.net/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cfbed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# news\n",
    "\"\"\"\n",
    "https://openbb.co/\n",
    "\n",
    "holidays\n",
    "    https://date.nager.at/\n",
    "    \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55821716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather\n",
    "\"\"\"\n",
    "https://www.7timer.info/doc.php?lang=en\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a600c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# social media api's - can we strip locations from somewhere? like tiktok....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68655f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo... build a couple libraries to hit the above APIs how we would like to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9311b415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ais",
   "language": "python",
   "name": "ais"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
