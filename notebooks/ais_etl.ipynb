{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b0df24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gdb_file = r\"D:\\data\\AIS\\global AIS from NOAA\\Zone10_2009_01\\Zone10_2009_01.gdb\"\n",
    "# outpath = r\"D:\\data\\AIS\\global AIS from NOAA\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63aa8d",
   "metadata": {},
   "source": [
    "# Read NOAA collected AIS data from url to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2202374e-914b-44a4-b491-ff6cc9d2b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"https://apps.dtic.mil/sti/trecms/pdf/AD1193822.pdf\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5f124c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "\"\"\" to make it run - make a req...txt and install later\n",
    "conda install gdal\n",
    "pip install h3 polars\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# import osgeo\n",
    "# from osgeo import ogrimport os\n",
    "\n",
    "import h3\n",
    "import polars as pl\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import urllib.request as urllib2\n",
    "\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "# thanks to https://stackoverflow.com/questions/11023530/python-to-list-http-files-and-directories\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b69dd8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2022/'\n",
    "ext = 'zip'\n",
    "\n",
    "# get the url for each file\n",
    "def listFD(url, ext=''):\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    return [url + '/' + node.get('href') for node in soup.find_all('a') if node.get('href').endswith(ext)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55aee8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add H3 feature and put timestamps in datetime format\n",
    "def pl_h3(pf, lat_col='LAT', lon_col='LON', new_col='H3', resolution=16):\n",
    "    \"\"\"\n",
    "    ### TODO: write a preprocessor function that uses this and re-saves the files\n",
    "    \n",
    "    this converts lat lon to h3 in polars\n",
    "    (function to apply a function to 2 columns in polars....)\n",
    "    \"\"\"\n",
    "    return pf.with_columns(pl.struct([lon_col,lat_col]).apply(lambda x: h3.geo_to_h3(lat=x[lat_col], lng=x[lon_col], resolution=7)).alias(new_col))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c8e421",
   "metadata": {},
   "source": [
    "## Data reading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e356fe91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "# this needs to go inside sav_dat()\n",
    "def read_gdb_from_zip(gdb_file):\n",
    "    \"\"\"\n",
    "    # if its a geodatabase (gdb)\n",
    "    ### Function to read very large gdb files into parquet\n",
    "\n",
    "    \"\"\"\n",
    "    def get_gdb_size(gdb_file):\n",
    "        \"\"\"\n",
    "        credit, modified from: \n",
    "            https://gis.stackexchange.com/questions/205861/get-row-counts-of-all-tables-in-file-geodatabase-ideally-from-metadata\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        #Opens filegdb using ogr driver\n",
    "        ogdb= ogr.Open(gdb_file)                         \n",
    "\n",
    "        #counts no. of feature classes in geodatabase\n",
    "        noOfLyrs = ogdb.GetLayerCount()  \n",
    "\n",
    "        layer = []\n",
    "        rows = []\n",
    "        #loop through feature classes\n",
    "        for fcIdx in range(0, noOfLyrs):              \n",
    "\n",
    "            #gets feature class\n",
    "            fc = ogdb.GetLayer(fcIdx)    \n",
    "\n",
    "            layer.append(fc.GetName())\n",
    "\n",
    "            rows.append(fc.GetFeatureCount())\n",
    "\n",
    "\n",
    "        return list(zip(layer, rows))\n",
    "\n",
    "    # layers = fiona.listlayers(gdb_file)\n",
    "    # print(layers)\n",
    "\n",
    "    import warnings; warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')\n",
    "\n",
    "    step = 100000\n",
    "    lat_col = 'LAT'\n",
    "    lon_col = 'LON'\n",
    "\n",
    "    layers = get_gdb_size(gdb_file)\n",
    "\n",
    "    for layer in layers:\n",
    "\n",
    "        start = 0\n",
    "        stop = step\n",
    "\n",
    "        rnd = 0\n",
    "\n",
    "        while start < layer[-1]:\n",
    "            rnd+=1\n",
    "\n",
    "            # dont read past the end of the file\n",
    "            if stop > layer[-1]:\n",
    "                stop = layer[-1]\n",
    "\n",
    "            print('Processing:', layer[0], '\\n\\t', start, '-', stop)\n",
    "\n",
    "            filename = f\"{egrps['gdb'][0].replace('.gdb', '')}_layer={layer[0]}_range={str(start)+'-'+str(stop)}_h3.parquet\"\n",
    "\n",
    "\n",
    "            print('\\tOutfile:', filename)\n",
    "            if filename in os.listdir(outpath):\n",
    "                print('\\t\\tFile already exists')\n",
    "            else:\n",
    "                try:\n",
    "                    gdf = gpd.read_file(gdb_file, \n",
    "                          rows=slice(start, stop-1), \n",
    "                          engine='fiona',\n",
    "                          layer=layer[0])\n",
    "\n",
    "                    # perform h3 conversion only where there is geometry to use\n",
    "                    if 'geometry' in gdf.columns and not gdf['geometry'].is_null().all():\n",
    "\n",
    "                        # keep only non-null geometries\n",
    "                        gdf = gdf.filter(pl.col('geometry').is_not_null())\n",
    "\n",
    "                        # extract coords from \n",
    "                        gdf[lon_col] = gdf['geometry'].apply(lambda x: x.x)\n",
    "                        gdf[lat_col] = gdf['geometry'].apply(lambda y: y.y)\n",
    "\n",
    "                        # now that we have read it in, ->polars->h3 encode->parquet file\n",
    "                        gdf = pl_h3(gdf, \n",
    "                                    lat_col=lat_col, \n",
    "                                    lon_col=lon_col, \n",
    "                                    new_col='H3', \n",
    "                                    resolution=16)\n",
    "\n",
    "                    else:\n",
    "                        filename = filename.replace('h3', 'NO_COORDS')\n",
    "                        print('\\tNo geometry')\n",
    "                except:\n",
    "                    print('  \\tFailed to process:', layer[0], '\\n\\t', start, stop)\n",
    "\n",
    "                # even if they have no geometry, still save the file\n",
    "                 # this is a crappy way to convert but all i could make work\n",
    "                gdf.to_parquet('file.parquet')\n",
    "                gdf = pl.read_parquet('file.parquet')\n",
    "\n",
    "                gdf.write_parquet(f'{outpath}{os.sep}{filename}')\n",
    "                print('\\tSuccessfully added file')\n",
    "\n",
    "            # set extents to iterate in the file\n",
    "            stop+=step\n",
    "            start+=step\n",
    "\n",
    "            if stop >= layer[-1]:\n",
    "                stop = layer[-1]\n",
    "\n",
    "            clear_output(wait=True)\n",
    "    print('Conversion Complete: ', gdb_file)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadb4bb7",
   "metadata": {},
   "source": [
    "## File reading and parsing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b12dc455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to select process by filetype - to add in above\n",
    "\n",
    "# Get gdb row counts so we can slice\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b88175c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'poster3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# https://www.rebasedata.com/python-read-gdb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# https://pypi.org/project/poster3/\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# https://github.com/dmorrison42/python-poster\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# dont actually need these at the moment but can use to stream it in rather than download the file. maybe\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mposter3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mencode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multipart_encode\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mposter3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaminghttp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_openers\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_dat\u001b[39m(url, outpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAIS\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mglobal AIS from NOAA\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# read a zip file from a url\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'poster3'"
     ]
    }
   ],
   "source": [
    "# https://www.rebasedata.com/python-read-gdb\n",
    "# https://pypi.org/project/poster3/\n",
    "# https://github.com/dmorrison42/python-poster\n",
    "\n",
    "# dont actually need these at the moment but can use to stream it in rather than download the file. maybe\n",
    "from poster3.encode import multipart_encode\n",
    "from poster3.streaminghttp import register_openers\n",
    "\n",
    "def save_dat(url, outpath = r\"D:\\data\\AIS\\global AIS from NOAA\"):\n",
    "\n",
    "    # read a zip file from a url\n",
    "    archive = urllib2.urlopen(url).read()\n",
    "    \n",
    "    # unzip file\n",
    "    archive = zipfile.ZipFile(BytesIO(archive))\n",
    "    \n",
    "    # get the filetypes so we can account for reading them all\n",
    "    # get any sub folders\n",
    "    dirs = list(set([os.path.dirname(x) for x in archive.namelist()]))\n",
    "\n",
    "    # if there are subfolders check their extensions\n",
    "    extens = list(set([d.split('.')[-1] for d in dirs]))\n",
    "\n",
    "    # group folders by extension\n",
    "    egrps = {e:[d for d in dirs if e in d] for e in extens}\n",
    "\n",
    "    # get folders without extensions....\n",
    "    fgrps = [d for d in dirs if d.split('.')[-1] not in extens]\n",
    "    \n",
    "    # sort the namelist by folder\n",
    "    \n",
    "    # match extensions to folders (really just for gdb)\n",
    "    \n",
    "    # based on folder type, read the file(s) and write to folder\n",
    "    \n",
    "    print('/tOpened zip from remote source:', )\n",
    "    \n",
    "    def read_csv_from_zip():\n",
    "        # if its a csv file\n",
    "        \"\"\"\n",
    "        output\n",
    "            h3 encoded parquet file from polars\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # loop through names and save one at a time to save memory\n",
    "            for f in range(len(archive.namelist())):\n",
    "                print('\\t\\tReading file:', archive.namelist()[f])\n",
    "                pl_h3(pl.read_csv(archive.open(archive.namelist()[f]), \n",
    "                                  encoding=\"utf8-lossy\"), \n",
    "                      lat_col='LAT', \n",
    "                      lon_col='LON', \n",
    "                      new_col='H3', \n",
    "                      resolution=16).write_parquet(f\"{outpath}{os.sep}{archive.namelist()[f].replace('.csv','_h3.parquet')}\")\n",
    "            print('\\t\\t\\tRead csv', f)\n",
    "        except:\n",
    "            print('\\t\\t\\tFailed csv', f)\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def read_to_file(name, ext='csv'):\n",
    "        \n",
    "        print('read_to_file() | NAME:', name)\n",
    "\n",
    "        if ext == 'csv':\n",
    "            print('\\t\\tCSV read')\n",
    "            return print('!!!1', read_csv_from_zip())\n",
    "        elif ext == 'gdb':\n",
    "            print('\\t\\tGDB read')\n",
    "            return print('!!!2', read_gdb_from_zip())\n",
    "\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        print('egrps', egrps)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print('fgrps', fgrps)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for e in egrps:\n",
    "        try:\n",
    "            read_to_file(egrps[e], ext=e)\n",
    "            print(e, 'Succeeded')\n",
    "        except:\n",
    "            print(e, 'Failed')\n",
    "    for f in fgrps:\n",
    "        try:\n",
    "            read_to_file(fgrps[f], ext=f)\n",
    "            print(f, 'Succeeded')\n",
    "        except:\n",
    "            print(f, 'Failed')\n",
    "    return archive, egrps, fgrps, extens\n",
    "    \n",
    "    \n",
    "    # what if its something else?!?!\n",
    "    \n",
    "        \n",
    "#                 \"\"\" \n",
    "#                 limit the fields if you want. not too big though so who cares\n",
    "#                 ['MMSI','BaseDateTime',\n",
    "#                 'LAT', 'LON','SOG','COG',\n",
    "#                 'Heading', 'VesselName', \n",
    "#                 'IMO', 'CallSign', \n",
    "#                 'VesselType', 'Status',\n",
    "#                 'Length','Width','Draft',\n",
    "#                 'Cargo', 'TransceiverClass']\n",
    "#                 \"\"\"\n",
    "            \n",
    "    return archive, egrps, fgrps\n",
    "\n",
    "# here is the test run\n",
    "archive, egrps, fgrps, extens = save_dat(listFD(url.replace('YEAR', str(2009)), ext='zip')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d8092b",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc476d25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files from: https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2009/\n",
      "/tOpened zip from remote source:\n",
      "egrps {'gdb': ['Zone10_2009_01.gdb']}\n",
      "fgrps []\n",
      "read_to_file() | NAME: gdb\n",
      "\t\tCSV read\n",
      "\t\tReading file: Zone10_2009_01.gdb/a00000001.gdbindexes\n",
      "\t\t\tFailed csv 0\n",
      "!!!1 False\n",
      "gdb Succeeded\n",
      "\t\t 2009 \n",
      "\t\t https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2009//01_January_2009/Zone10_2009_01.zip Failed\n",
      "Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Polars found a filename. Ensure you pass a path to the file instead of a python file object when possible for best performance."
     ]
    }
   ],
   "source": [
    "# download AIS data from https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2022/\n",
    "\n",
    "for i in range(2009, 2022): # these are the available years, 2009-2022\n",
    "    \n",
    "    # read file names\n",
    "    url = f'https://coast.noaa.gov/htdata/CMSP/AISDataHandler/{i}/'\n",
    "    \n",
    "    print(f\"Reading files from:\", url)\n",
    "    \n",
    "    urls = listFD(url, ext='zip') # this gets the zip files\n",
    "\n",
    "    # get the data\n",
    "    for u in urls:\n",
    "        try:\n",
    "            df = pl_h3(pl.from_pandas(save_dat(u)), \n",
    "                       lat_col='LAT', \n",
    "                       lon_col='LON', \n",
    "                       new_col='H3', \n",
    "                       resolution=16)\n",
    "            \n",
    "            print('\\t\\t',i,'\\n\\t\\t',u, 'Success')\n",
    "            \n",
    "            df.write_parquet(filename)\n",
    "            \n",
    "        except:\n",
    "            print('\\t\\t',i,'\\n\\t\\t', u, 'Failed')\n",
    "        \n",
    "        print('Test')\n",
    "        break\n",
    "\n",
    "    # log/store our own minimized and enriched\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d89275dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tOpened zip from remote source:\n",
      "\t\tReading file: Zone10_2009_01.gdb/a00000001.gdbindexes\n",
      "\t\t\tFailed csv 0\n",
      "False\n",
      "gdb Succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Polars found a filename. Ensure you pass a path to the file instead of a python file object when possible for best performance."
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<zipfile.ZipFile file=<_io.BytesIO object at 0x7fef118efd10> mode='r'>,\n",
       " {'gdb': ['Zone10_2009_01.gdb']},\n",
       " [],\n",
       " ['gdb'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = save_dat(u)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2489b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to read folders by extension\n",
    "for ex in extens:\n",
    "    if ex in egrps.keys():\n",
    "        print(True)\n",
    "        pass\n",
    "    elif ex in fgrps.keys():\n",
    "        print(True)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a947f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Register the streaming http handlers with urllib2\n",
    "# register_openers()\n",
    "\n",
    "# # Use multipart encoding for the input files\n",
    "# datagen, headers = multipart_encode({ 'files[]': open('Zone10_2009_01.gdb', 'rb')})\n",
    "\n",
    "# datagen, headers = multipart_encode({ 'files[]': open(u, 'rb')})\n",
    "\n",
    "# # Create the request object\n",
    "# request = urllib2.Request(u, datagen, headers)\n",
    "\n",
    "# # Do the request and get the response\n",
    "# # Here the GDB file gets converted to CSV\n",
    "# response = urllib2.urlopen(request)\n",
    "\n",
    "# # Check if an error came back\n",
    "# if response.info().getheader('Content-Type') == 'application/json':\n",
    "#     print response.read()\n",
    "#     sys.exit(1)\n",
    "\n",
    "# # Write the response to /tmp/output.zip\n",
    "# with open('/tmp/output.zip', 'wb') as local_file:\n",
    "#     local_file.write(response.read())\n",
    "\n",
    "# print 'Conversion result successfully written to /tmp/output.zip!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca8cf29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf = gpd.read_file(gdb_file, \n",
    "#                   rows=slice(int(start), int(stop-1)), \n",
    "#                   engine='fiona',\n",
    "#                   layer=layer)\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47464a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_h3(pl.from_numpy(gdf.to_numpy(), \n",
    "                    columns=list(gdf.columns)), # this is atrocious, triple conversion, but cant be helped til geopolars and geoparquet work\n",
    "      lat_col='LAT', \n",
    "      lon_col='LON', \n",
    "      new_col='H3', \n",
    "      resolution=16).write_parquet(f\"{outpath}{os.sep}{archive.namelist()[f].replace('.csv','_h3.parquet')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadbd931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow as pa\n",
    "\n",
    "# import geopolars as gpl\n",
    "\n",
    "# reader = pa.ipc.open_file(gdb_file)\n",
    "# table = reader.read_all()\n",
    "\n",
    "# df = gpl.from_arrow(table)\n",
    "# geom = df.get_column(\"geometry\")\n",
    "# out = geom.centroid()\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "728fe10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\data\\\\AIS\\\\global AIS from NOAA\\\\Zone10_2009_01\\\\Zone10_2009_01.gdb'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdb_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0555d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parliament",
   "language": "python",
   "name": "parliament"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
