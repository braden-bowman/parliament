{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0df24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gdb_file = r\"D:\\data\\AIS\\global AIS from NOAA\\Zone10_2009_01\\Zone10_2009_01.gdb\"\n",
    "# outpath = r\"D:\\data\\AIS\\global AIS from NOAA\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63aa8d",
   "metadata": {},
   "source": [
    "# Read NOAA collected AIS data from url to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2202374e-914b-44a4-b491-ff6cc9d2b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"https://apps.dtic.mil/sti/trecms/pdf/AD1193822.pdf\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd91cf3-b42c-426a-a1dc-3bbbd5b45492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install gdal osgeo\n",
    "\n",
    "# !sudo add-apt-repository ppa:ubuntugis/ppa\n",
    "# !sudo apt-get update\n",
    "# !sudo apt-get install gdal-bin\n",
    "# !sudo apt-get install libgdal-dev\n",
    "# !export CPLUS_INCLUDE_PATH=/usr/include/gdal\n",
    "# !export C_INCLUDE_PATH=/usr/include/gdal\n",
    "# !pip install GDAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f124c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "\"\"\" to make it run - make a req...txt and install later\n",
    "conda install gdal\n",
    "pip install h3 polars\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# thanks to https://stackoverflow.com/questions/11023530/python-to-list-http-files-and-directories\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request as urllib2\n",
    "\n",
    "import h3\n",
    "import polars as pl\n",
    "\n",
    "# https://discuss.python.org/t/trying-to-scrape-and-download-zipfiles/35399/2\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "import fiona\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca22ba66-42da-41d5-a9a7-171544667fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # add H3 feature and put timestamps in datetime format\n",
    "#     def pl_h3(pf, lat_col='LAT', lon_col='LON', new_col='H3', resolution=16):\n",
    "#         \"\"\"\n",
    "#         ### TODO: write a preprocessor function that uses this and re-saves the files\n",
    "\n",
    "#         this converts lat lon to h3 in polars\n",
    "#         (function to apply a function to 2 columns in polars....)\n",
    "#         \"\"\"\n",
    "#         return pf.with_columns(pl.struct([lon_col,lat_col]).apply(lambda x: h3.geo_to_h3(lat=x[lat_col], lng=x[lon_col], resolution=7)).alias(new_col))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c8e421",
   "metadata": {},
   "source": [
    "## Data reading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e356fe91",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dadb4bb7",
   "metadata": {},
   "source": [
    "## File reading and parsing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a9b37f-c216-4394-8e84-34acd505767c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.rebasedata.com/python-read-gdb\n",
    "# https://pypi.org/project/poster3/\n",
    "# https://github.com/dmorrison42/python-poster\n",
    "\n",
    "# dont actually need these at the moment but can use to stream it in rather than download the file. maybe\n",
    "from poster3.encode import multipart_encode\n",
    "from poster3.streaminghttp import register_openers\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# class NoaaApi:\n",
    "#     def __init__(\n",
    "#         self\n",
    "#     ):\n",
    "\n",
    "\n",
    "def get_http_filestructure(\n",
    "    URL,\n",
    "    DATA_FOLDER = '/mnt/d/data/AIS',\n",
    "    EXTENSION = 'zip',\n",
    "    test = False\n",
    "):\n",
    "    # get the url for each file\n",
    "    def listFD(\n",
    "        url, \n",
    "        ext='zip'\n",
    "    ):\n",
    "        page = requests.get(url).text\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        files = [node.get('href') for node in soup.find_all('a')]\n",
    "\n",
    "        return [f'{url}{os.sep}{file}' \n",
    "                for file in files \n",
    "                if file.endswith(ext)]\n",
    "    \n",
    "    def http_zip_to_parquet(\n",
    "        file,\n",
    "        out_folder\n",
    "    ):\n",
    "        # if its a csv file\n",
    "        \"\"\"\n",
    "        output\n",
    "            h3 encoded parquet file from polars\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # loop through names and save one at a time to save memory\n",
    "            for f in range(len(file.namelist())):\n",
    "                print('\\t\\tReading file:', file.namelist()[f])\n",
    "                \n",
    "                # add H3 cell encoding and write to file\n",
    "                (\n",
    "                    pl.read_csv(\n",
    "                        file.open(file.namelist()[f]), \n",
    "                        encoding=\"utf8-lossy\")\n",
    "                .write_parquet(\n",
    "                    f\"{out_folder}{os.sep}{file.namelist()[f].replace('.csv','.parquet')}\")\n",
    "                )\n",
    "                \n",
    "            print('\\t\\t\\tWrote to file:', f)\n",
    "        except:\n",
    "            print('\\t\\t\\tFailed .reading csv from .zip', f)\n",
    "        \n",
    "        return True\n",
    "\n",
    "    print('Reading files from:', URL)\n",
    "    \n",
    "    try:\n",
    "        outpath = f\"{DATA_FOLDER}{os.sep}\" #r\"D:\\data\\AIS\\global AIS from NOAA\"\n",
    "\n",
    "        # read a zip file from a url\n",
    "        archive = urllib2.urlopen(URL).read()\n",
    "        archive =  listFD(URL, ext=EXTENSION)\n",
    "        \n",
    "        for file_url in archive:\n",
    "            print('\\tDownloading:', file_url)\n",
    "            \n",
    "            # unzip file\n",
    "            zip_file = requests.get(file_url)\n",
    "        \n",
    "            if zip_file.status_code == 200:\n",
    "                zip_file = zipfile.ZipFile(BytesIO(zip_file.content))\n",
    "            else:\n",
    "                print('\\tRequest failed.')\n",
    "                continue\n",
    "            \n",
    "            # get any sub folders\n",
    "            print('\\tGetting directories...')\n",
    "            dirs = list(set([os.path.dirname(x) for x in zip_file.namelist()]))\n",
    "            \n",
    "            # if there are subfolders check their extensions\n",
    "            print('\\tGetting file extensions...')\n",
    "            extens = list(set([d.split('.')[-1] for d in dirs]))\n",
    "            \n",
    "            # group folders by extension\n",
    "            print('\\tGrouping files...')\n",
    "            egrps = {e:[d for d in dirs if e in d] for e in extens}\n",
    "            \n",
    "            # get folders without extensions....\n",
    "            fgrps = [d for d in dirs if d.split('.')[-1] not in extens]\n",
    "\n",
    "            ####TODO: # sort the namelist by folder\n",
    "            ####TODO: # match extensions to folders (really just for gdb)\n",
    "            ####TODO: # based on folder type, read the file(s) and write to folder\n",
    "            \n",
    "            return {'dirs':dirs, 'extens':extens, 'egrps':egrps, 'fgrps':fgrps, 'zip_file':zip_file}\n",
    "            # data = http_zip_to_parquet(, DATA_FOLDER)\n",
    "            \n",
    "            # return zip_file\n",
    "        \n",
    "    except:\n",
    "        print(f'\\tFailed url parsing: {file_url}')\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        'outpath':outpath,\n",
    "        'archive':archive, \n",
    "        'dirs':dirs, \n",
    "        'estens':estens, \n",
    "        'egrps':egrps, \n",
    "        'fgrps':fgrps\n",
    "    }\n",
    "    \n",
    "    # print('/tSuccessfully opened zip from remote source')\n",
    "#     if test == True:\n",
    "#         try:\n",
    "#             print('egrps', egrps)\n",
    "#         except:\n",
    "#             pass\n",
    "#         try:\n",
    "#             print('fgrps', fgrps)\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "#           for e in egrps:\n",
    "#         try:\n",
    "#             read_to_file(egrps[e], ext=e)\n",
    "#             print(e, 'Succeeded')\n",
    "          \n",
    "          \n",
    "#         except:\n",
    "#             print(e, 'Failed')\n",
    "#     for f in fgrps:\n",
    "#         try:\n",
    "#             read_to_file(fgrps[f], ext=f)\n",
    "#             print(f, 'Succeeded')\n",
    "#         except:\n",
    "#             print(f, 'Failed')\n",
    "              \n",
    "#     return archive, egrps, fgrps, extens\n",
    "    \n",
    "    \n",
    "#     # what if its something else?!?!\n",
    "    \n",
    "        \n",
    "# #                 \"\"\" \n",
    "# #                 limit the fields if you want. not too big though so who cares\n",
    "# #                 ['MMSI','BaseDateTime',\n",
    "# #                 'LAT', 'LON','SOG','COG',\n",
    "# #                 'Heading', 'VesselName', \n",
    "# #                 'IMO', 'CallSign', \n",
    "# #                 'VesselType', 'Status',\n",
    "# #                 'Length','Width','Draft',\n",
    "# #                 'Cargo', 'TransceiverClass']\n",
    "# #                 \"\"\"\n",
    "            \n",
    "#     return archive, egrps, fgrps\n",
    "\n",
    "\n",
    "# url = 'https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2022/'\n",
    "url = 'https://coast.noaa.gov/htdata/CMSP/AISDataHandler/#YEAR#/'\n",
    "# ext = 'zip'\n",
    "\n",
    "# here is the test run\n",
    "# archive, egrps, fgrps, extens = save_dat(listFD(url.replace('YEAR', str(2009)), ext='zip')[0])\n",
    "# f = \n",
    "# archive, egrps, fgrps, extens = \n",
    "ff = get_http_filestructure(url.replace('#YEAR#', str(2009)))\n",
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50792b02-c0a7-468c-b1fc-f9218f9394ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff['zip_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a9bd94-370b-4f2b-8983-0469cf30457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = list(set([os.path.dirname(x) for x in zz.namelist()]))\n",
    "\n",
    "extens = list(set([d.split('.')[-1] for d in dirs]))\n",
    "\n",
    "egrps = {e:[d for d in dirs if e in d] for e in extens}\n",
    "\n",
    "fgrps = [d for d in dirs if d.split('.')[-1] not in extens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1673ed-3e36-4f79-85e4-8c11b79cf242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def http_zip_to_parquet(\n",
    "    file,\n",
    "    out_folder\n",
    "):\n",
    "    # if its a csv file\n",
    "    \"\"\"\n",
    "    output\n",
    "        h3 encoded parquet file from polars\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # loop through names and save one at a time to save memory\n",
    "        for f in range(len(file.namelist())):\n",
    "            print('\\t\\tReading file:', file.namelist()[f])\n",
    "\n",
    "            # add H3 cell encoding and write to file\n",
    "            (\n",
    "                pl.read_csv(\n",
    "                    file.open(file.namelist()[f]), \n",
    "                    encoding=\"utf8-lossy\")\n",
    "            .write_parquet(\n",
    "                f\"{out_folder}{os.sep}{file.namelist()[f].replace('.csv','.parquet')}\")\n",
    "            )\n",
    "\n",
    "        print('\\t\\t\\tWrote to file:', f)\n",
    "    except:\n",
    "        print('\\t\\t\\tFailed .reading csv from .zip', f)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a388e088-a40f-4780-8ced-8317227ac412",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# this needs to go inside sav_dat()\n",
    "def read_gdb_from_zip(\n",
    "    gdb_file,\n",
    "    DATA_FOLDER = '/mnt/d/data/AIS',\n",
    "):\n",
    "    \"\"\"\n",
    "    # if its a geodatabase (gdb)\n",
    "    ### Function to read very large gdb files into parquet\n",
    "\n",
    "    \"\"\"\n",
    "    def get_gdb_size(gdb_file):\n",
    "        \"\"\"\n",
    "        credit, modified from: \n",
    "            https://gis.stackexchange.com/questions/205861/get-row-counts-of-all-tables-in-file-geodatabase-ideally-from-metadata\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        #Opens filegdb using ogr driver\n",
    "        ogdb= ogr.Open(gdb_file)                         \n",
    "\n",
    "        #counts no. of feature classes in geodatabase\n",
    "        noOfLyrs = ogdb.GetLayerCount()  \n",
    "\n",
    "        layer = []\n",
    "        rows = []\n",
    "        #loop through feature classes\n",
    "        for fcIdx in range(0, noOfLyrs):              \n",
    "\n",
    "            #gets feature class\n",
    "            fc = ogdb.GetLayer(fcIdx)    \n",
    "\n",
    "            layer.append(fc.GetName())\n",
    "\n",
    "            rows.append(fc.GetFeatureCount())\n",
    "\n",
    "\n",
    "        return list(zip(layer, rows))\n",
    "\n",
    "    # layers = fiona.listlayers(gdb_file)\n",
    "    # print(layers)\n",
    "\n",
    "    import warnings; warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')\n",
    "\n",
    "    step = 100000\n",
    "    # lat_col = 'LAT'\n",
    "    # lon_col = 'LON'\n",
    "\n",
    "    layers = get_gdb_size(gdb_file)\n",
    "\n",
    "    for layer in layers:\n",
    "\n",
    "        start = 0\n",
    "        stop = step\n",
    "\n",
    "        rnd = 0\n",
    "\n",
    "        while start < layer[-1]:\n",
    "            rnd+=1\n",
    "\n",
    "            # dont read past the end of the file\n",
    "            if stop > layer[-1]:\n",
    "                stop = layer[-1]\n",
    "\n",
    "            print('Processing:', layer[0], '\\n\\t', start, '-', stop)\n",
    "\n",
    "            filename = f\"{egrps['gdb'][0].replace('.gdb', '')}_layer={layer[0]}_range={str(start)+'-'+str(stop)}_h3.parquet\"\n",
    "\n",
    "\n",
    "            print('\\tOutfile:', filename)\n",
    "            \n",
    "            if filename in os.listdir(DATA_FOLDER):\n",
    "                print('\\t\\tFile already exists')\n",
    "            else:\n",
    "                try:\n",
    "                    gdf = gpd.read_file(gdb_file, \n",
    "                          rows=slice(start, stop-1), \n",
    "                          engine='fiona',\n",
    "                          layer=layer[0])\n",
    "\n",
    "                    # perform h3 conversion only where there is geometry to use\n",
    "                    if 'geometry' in gdf.columns and not gdf['geometry'].is_null().all():\n",
    "\n",
    "                        # keep only non-null geometries\n",
    "                        gdf = gdf.filter(pl.col('geometry').is_not_null())\n",
    "\n",
    "                        # extract coords from \n",
    "                        gdf[lon_col] = gdf['geometry'].apply(lambda xx: xx.x)\n",
    "                        gdf[lat_col] = gdf['geometry'].apply(lambda yy: yy.y)\n",
    "\n",
    "                        # now that we have read it in, ->polars->h3 encode->parquet file\n",
    "                        # gdf = pl_h3(gdf, \n",
    "                        #             lat_col=lat_col, \n",
    "                        #             lon_col=lon_col, \n",
    "                        #             new_col='H3', \n",
    "                        #             resolution=16)\n",
    "\n",
    "                    else:\n",
    "                        print('\\tNo geometry:', gdb_file, start, stop)\n",
    "                except:\n",
    "                    print('  \\tFailed to process:', layer[0], '\\n\\t', start, stop)\n",
    "\n",
    "                # even if they have no geometry, still save the file\n",
    "                 # this is a crappy way to convert but all i could make work\n",
    "                \n",
    "                file_path = f\"\"\"{DATA_FOLDER}{os.sep}{gdb_file.replace('gdb', 'parquet')}\"\"\"\n",
    "                    \n",
    "                gdf.to_parquet(file_path)\n",
    "                # gdf = pl.read_parquet(file_path) # do we really need this step????\n",
    "                gdf.write_parquet(file_path)\n",
    "                                  \n",
    "                print('\\tSuccessfully added file')\n",
    "\n",
    "            # set extents to iterate in the file\n",
    "            stop+=step\n",
    "            start+=step\n",
    "\n",
    "            if stop >= layer[-1]:\n",
    "                stop = layer[-1]\n",
    "\n",
    "            clear_output(wait=True)\n",
    "    print('Conversion Complete: ', gdb_file)\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "DATA_FOLDER = '/mnt/d/data/AIS'\n",
    "\n",
    "df = read_gdb_from_zip(ff['zip_file'])\n",
    "\n",
    "# http_zip_to_parquet(ff['zip_file'], DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab6e556-36d1-4b30-96d1-cad02a99fa31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!conda install gdal -y\n",
    "!conda install -c conda-forge gdal -y\n",
    "!conda install -c conda-forge osgeo -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "023e1561-614b-40d9-b3e5-dd67e91a2cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export PYTHONPATH=$PYTHONPATH:/home/ubuntu/anaconda3/lib/python3.9/site-packages/osgeo/_gdal.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ede7e-5a7e-4349-a3d9-05b83bcdb132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from osgeo import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1efbc67a-56d7-486c-aa79-79477ec8aa98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gdal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgdal\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# ff['zip_file']\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m## /home/ubuntu/anaconda3/lib/python3.9/site-packages/osgeo/gdal.py\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m## /home/ubuntu/anaconda3/lib/python3.9/site-packages/osgeo/_gdal.py\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gdal'"
     ]
    }
   ],
   "source": [
    "import gdal\n",
    "# ff['zip_file']\n",
    "\n",
    "## /home/ubuntu/anaconda3/lib/python3.9/site-packages/osgeo/gdal.py\n",
    "## /home/ubuntu/anaconda3/lib/python3.9/site-packages/osgeo/_gdal.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c222f0dc-2b34-44b4-9b3a-1c8ba518a0fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !conda install gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0336ff-6358-4ac7-b5b6-be9fbc5dd481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install osgeo -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c8087-554d-491a-9153-9c0fc6a89646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import osgeo\n",
    "from osgeo import ogr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc47b3b-d178-440b-afbe-52b86c1ff65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '/mnt/d/data/AIS'\n",
    "read_gdb_from_zip(zz, DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f5357-4cb4-42e9-9347-c32cf3df337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install osgeo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d8092b",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc476d25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download AIS data from https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2022/\n",
    "\n",
    "for i in range(2009, 2022): # these are the available years, 2009-2022\n",
    "    print('Downloading year:\", i\n",
    "    # read file names\n",
    "    url = f'https://coast.noaa.gov/htdata/CMSP/AISDataHandler/{i}/'\n",
    "    \n",
    "    print(f\"Reading files from:\", url)\n",
    "    \n",
    "    urls = listFD(url, ext='zip') # this gets the zip files\n",
    "\n",
    "    # get the data\n",
    "    for u in urls:\n",
    "        print('\\tdownloading:', u\n",
    "        try:\n",
    "            df = pl_h3(\n",
    "                pl.from_pandas(save_dat(u)), \n",
    "                lat_col='LAT', \n",
    "                lon_col='LON', \n",
    "                new_col='H3', \n",
    "                resolution=16)\n",
    "            \n",
    "            print('\\t\\t',i,'\\n\\t\\t',u, 'Success')\n",
    "            \n",
    "            df.write_parquet(filename)\n",
    "            \n",
    "        except:\n",
    "            print('\\t\\t',i,'\\n\\t\\t', u, 'Failed')\n",
    "        \n",
    "        print('Test')\n",
    "        break\n",
    "\n",
    "    # log/store our own minimized and enriched\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58265aab-42d3-4bb2-baaf-8151370810e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89275dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = save_dat(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2489b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to read folders by extension\n",
    "for ex in extens:\n",
    "    if ex in egrps.keys():\n",
    "        print(True)\n",
    "        pass\n",
    "    elif ex in fgrps.keys():\n",
    "        print(True)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a947f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Register the streaming http handlers with urllib2\n",
    "# register_openers()\n",
    "\n",
    "# # Use multipart encoding for the input files\n",
    "# datagen, headers = multipart_encode({ 'files[]': open('Zone10_2009_01.gdb', 'rb')})\n",
    "\n",
    "# datagen, headers = multipart_encode({ 'files[]': open(u, 'rb')})\n",
    "\n",
    "# # Create the request object\n",
    "# request = urllib2.Request(u, datagen, headers)\n",
    "\n",
    "# # Do the request and get the response\n",
    "# # Here the GDB file gets converted to CSV\n",
    "# response = urllib2.urlopen(request)\n",
    "\n",
    "# # Check if an error came back\n",
    "# if response.info().getheader('Content-Type') == 'application/json':\n",
    "#     print response.read()\n",
    "#     sys.exit(1)\n",
    "\n",
    "# # Write the response to /tmp/output.zip\n",
    "# with open('/tmp/output.zip', 'wb') as local_file:\n",
    "#     local_file.write(response.read())\n",
    "\n",
    "# print 'Conversion result successfully written to /tmp/output.zip!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca8cf29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf = gpd.read_file(gdb_file, \n",
    "#                   rows=slice(int(start), int(stop-1)), \n",
    "#                   engine='fiona',\n",
    "#                   layer=layer)\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47464a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_h3(pl.from_numpy(gdf.to_numpy(), \n",
    "                    columns=list(gdf.columns)), # this is atrocious, triple conversion, but cant be helped til geopolars and geoparquet work\n",
    "      lat_col='LAT', \n",
    "      lon_col='LON', \n",
    "      new_col='H3', \n",
    "      resolution=16).write_parquet(f\"{outpath}{os.sep}{archive.namelist()[f].replace('.csv','_h3.parquet')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadbd931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow as pa\n",
    "\n",
    "# import geopolars as gpl\n",
    "\n",
    "# reader = pa.ipc.open_file(gdb_file)\n",
    "# table = reader.read_all()\n",
    "\n",
    "# df = gpl.from_arrow(table)\n",
    "# geom = df.get_column(\"geometry\")\n",
    "# out = geom.centroid()\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728fe10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0555d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parliament",
   "language": "python",
   "name": "parliament"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
